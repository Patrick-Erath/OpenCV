{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>ECSE 415 - Assignment 2</center>\n",
    "  ### <center>Patrick Erath - 260719203 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question3 - Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 - Resize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images = []\n",
    "\n",
    "# TODO: check if grayscale is correct for HoG\n",
    "\n",
    "# Read in each image, convert to Gray and resize, then append to list\n",
    "for i in range(1,15):\n",
    "    img_temp = cv2.imread(\"./car\"+str(i)+\".jpg\")\n",
    "    img_temp = cv2.cvtColor(img_temp, cv2.COLOR_BGR2GRAY) \n",
    "    img_temp = cv2.resize(img_temp, (128,128))\n",
    "    images.append(img_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 - Compute HoG features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function which takes list of images as arguments\n",
    "# and delivers list of HoG features as output. \n",
    "\n",
    "def hog_features(imgs_arr, cell_size=(4,4), block_size=(2,2), nbins=9):\n",
    "    hog_feats_arr = []\n",
    "    count = 0\n",
    "    for img in imgs_arr:\n",
    "        # Check window size\n",
    "        if(img.shape[0]%16!=0 or img.shape[1]%16!=0):\n",
    "            raise Exception('Invalid Image Size')\n",
    "        else:\n",
    "            # Compute window size\n",
    "            win_XY = img.shape[0] // cell_size[0] * cell_size[1]\n",
    "            \n",
    "            # Compute blocks\n",
    "            block_XY = block_size[0] * cell_size[0]\n",
    "            \n",
    "            # Create HoG object\n",
    "            hog = cv2.HOGDescriptor(_winSize = (win_XY, win_XY),\n",
    "                                    _blockSize = (block_XY, block_XY),\n",
    "                                    _blockStride = (cell_size[1], cell_size[0]),\n",
    "                                    _cellSize = (cell_size[1], cell_size[0]),\n",
    "                                    _nbins = nbins\n",
    "                                   )\n",
    "            \n",
    "            # Compute number of cells \n",
    "            n_cells = (img.shape[0] // cell_size[0], img.shape[1] // cell_size[1])\n",
    "            \n",
    "            # Compute HoG features\n",
    "            hog_feats = hog.compute(img) \\\n",
    "                            .reshape(n_cells[1] - block_size[1] + 1,\n",
    "                                     n_cells[0] - block_size[0] +1, \n",
    "                                     block_size[1], block_size[0], nbins) \\\n",
    "                            .transpose((1, 0, 3, 2, 4))\n",
    "            \n",
    "            hog_feats_arr.append(hog_feats)\n",
    "            \n",
    "    return hog_feats_arr\n",
    "            \n",
    "feats = hog_features(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 - Calculate mean feature map across training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the HoGs\n",
    "# Do we iterate over each HoG and take the average??\n",
    "\n",
    "# Sum the features of the 14 trained images\n",
    "feats_sum = feats[0] + feats[1] + feats[2] + feats[3] + feats[4] + feats[5] + feats[6] + feats[7] + feats[8]  \\\n",
    "            + feats[9] + feats[10] + + feats[11] + feats[12] + feats[13]\n",
    "\n",
    "feats_mean = feats_sum / 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 - Repeat steps 3.1.2 - 3.1.3 for images flipped vertically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_flip = []\n",
    "\n",
    "# Flip image about the vertical axis -> ie across y-axis\n",
    "for img in images:\n",
    "    imgs_flip.append(cv2.flip(img, 1))\n",
    "    \n",
    "feats_flip = hog_features(imgs_flip)\n",
    "\n",
    "# Sum the features of the 14 trained images\n",
    "feats_sum = feats_flip[0] + feats_flip[1] + feats_flip[2] + feats_flip[3] + feats_flip[4] + feats_flip[5] \\\n",
    "            + feats_flip[6] + feats_flip[7] + feats_flip[8] + feats[9] + feats[10] + feats[11] \\\n",
    "            + feats[12] + feats[13]\n",
    "\n",
    "feats_flip_mean = feats_sum / 14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 - Display 9 orientation channels for first block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "first_block = feats_mean[:, :, 0,0,0]\n",
    "\n",
    "# Normalize array for display\n",
    "normalized_array = np.reshape(first_block, (1,31*31))\n",
    "\n",
    "# Display mean feature map with 9 orientation channels\n",
    "plt.hist(normalized_array[0], 9, color='b')\n",
    "plt.title(\"Orientation Channels for first block\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 - Extract overlapping windows from test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # TODO: REMOVE\n",
    "\n",
    "test_img = cv2.imread('test.jpg')\n",
    "test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Step size is the \"step\" (or over lap) when sliding \n",
    "# an overlapping 128x128 window across the image\n",
    "stepSize = 32\n",
    "winX = (test_img.shape[1] // stepSize) * stepSize\n",
    "winY = (test_img.shape[0] // stepSize) * stepSize\n",
    "\n",
    "test_img = cv2.resize(test_img, (winX, winY))\n",
    "\n",
    "windows_array = []\n",
    "\n",
    "def sliding_window(image, stepSize, windowSize=(128,128)):\n",
    "    # Sliding the window across the image\n",
    "    for y in range(0, image.shape[0], stepSize):\n",
    "        for x in range(0, image.shape[1], stepSize):\n",
    "            yield (x, y, image[y:y + windowSize[1], x:x + windowSize[0]])\n",
    "\n",
    "\n",
    "# 1. Create a sliding window by sliding a 128x128 window across the image\n",
    "for (x, y, window) in sliding_window(test_img, stepSize):\n",
    "    if window.shape[0] != 128 or window.shape[1] != 128:\n",
    "        continue\n",
    "    else:\n",
    "        # Append the given window to a window array\n",
    "        windows_array.append(window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 - Compute HoG features for each window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Resizing the windows to 128x128 is not necessary because \n",
    "# a sliding window of 128x128 was used. Thus the windows are already\n",
    "# of size 128x128\n",
    "\n",
    "window_feats = hog_features(windows_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 - Compute Euclidean Distance  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the euclidean distance between the feature map of each\n",
    "# window and the mean feature map of training images\n",
    "\n",
    "# For non-flipped images\n",
    "euclidean_distance = []\n",
    "\n",
    "#print(feats_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 - Threshold the distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5 - Experiment with the size of the window, stride and detection threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each block (0,0), (0,1), (1,0) and (1,1)\n",
    "# Iterate over each pixel, and find the average of that pixel for\n",
    "# the 14 trained image\n",
    "# Final array is a nested array with the average pixel for each block\n",
    "for l in range(2):\n",
    "    for k in range(2):\n",
    "        block_avg = np.zeros((height, width))\n",
    "        for h in range(height):\n",
    "            pixel_sum = 0\n",
    "            for w in range(width):\n",
    "                for i in range(len(feats)):\n",
    "                    pixel_sum = feats[i][:,:,k,l,0][h][w]\n",
    "                pixel_avg = pixel_sum / (len(feats))\n",
    "                block_avg[w][h]= pixel_avg\n",
    "                \n",
    "        blocks_avg.append(block_avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
